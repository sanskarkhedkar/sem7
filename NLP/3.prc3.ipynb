{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:47:17.318361Z",
     "start_time": "2025-07-31T04:47:17.255554Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df8453589b0f28f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:46:55.903906Z",
     "start_time": "2025-07-31T04:46:55.893037Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_doc(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = \" \"\n",
    "    for para in doc.paragraphs:\n",
    "        full_text += para.text + \" \"\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db812375c75e79be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:22:27.247637Z",
     "start_time": "2025-07-31T04:22:27.238985Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e77aff00381cecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:34:14.016351Z",
     "start_time": "2025-07-31T04:34:13.931624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies:\n",
      "\n",
      "natural: 1\n",
      "language: 2\n",
      "processing: 1\n",
      "nlp: 4\n",
      "is: 1\n",
      "a: 1\n",
      "field: 1\n",
      "of: 2\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "helps: 1\n",
      "computers: 1\n",
      "understand: 1\n",
      "interpret: 1\n",
      "and: 3\n",
      "generate: 1\n",
      "human: 1\n",
      "applications: 1\n",
      "include: 1\n",
      "chatbots: 1\n",
      "search: 1\n",
      "engines: 1\n",
      "sentiment: 1\n",
      "analysis: 2\n",
      "learning: 1\n",
      "requires: 1\n",
      "practice: 1\n",
      "with: 1\n",
      "text: 1\n",
      "data: 1\n",
      "tokenization: 1\n",
      "frequency: 1\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/sansk/Desktop/sample.docx\"\n",
    "text = read_doc(file_path)\n",
    "words = tokenize(text)\n",
    "word_freq = Counter(words)\n",
    "print(\"Word Frequencies:\\n\")\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f14af5bca51068d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:50:04.737044Z",
     "start_time": "2025-07-31T04:50:04.568021Z"
    }
   },
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dae3df762dd6df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:50:05.377136Z",
     "start_time": "2025-07-31T04:50:05.355537Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_doc(pdf_path):\n",
    "    doc1 = fitz.open(pdf_path)\n",
    "    full_text1 = \" \"\n",
    "    for page in doc1:\n",
    "        full_text1 += page.get_text() + \" \"\n",
    "    return full_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6e58fb79ffeda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:50:05.861772Z",
     "start_time": "2025-07-31T04:50:05.856229Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words1 = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a393402677ad240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:50:06.503989Z",
     "start_time": "2025-07-31T04:50:06.405743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies:\n",
      "\n",
      "natural: 1\n",
      "language: 2\n",
      "processing: 1\n",
      "nlp: 4\n",
      "is: 1\n",
      "a: 1\n",
      "field: 1\n",
      "of: 2\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "helps: 1\n",
      "computers: 1\n",
      "understand: 1\n",
      "interpret: 1\n",
      "and: 3\n",
      "generate: 1\n",
      "human: 1\n",
      "applications: 1\n",
      "include: 1\n",
      "chatbots: 1\n",
      "search: 1\n",
      "engines: 1\n",
      "sentiment: 1\n",
      "analysis: 2\n",
      "learning: 1\n",
      "requires: 1\n",
      "practice: 1\n",
      "with: 1\n",
      "text: 1\n",
      "data: 1\n",
      "tokenization: 1\n",
      "frequency: 1\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/sansk/Desktop/sample.pdf\"\n",
    "text1 = read_doc(file_path)\n",
    "words1 = tokenize(text)\n",
    "word_freq = Counter(words1)\n",
    "print(\"Word Frequencies:\\n\")\n",
    "for word1, freq1 in word_freq.items():\n",
    "    print(f\"{word1}: {freq1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
